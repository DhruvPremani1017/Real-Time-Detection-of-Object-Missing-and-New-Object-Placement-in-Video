{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11601370,"sourceType":"datasetVersion","datasetId":7275971},{"sourceId":18802,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":15593,"modelId":23664}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, glob, random, pathlib, pprint\n\nDATA_DIR = pathlib.Path(\"/kaggle/input/data-set-s/MOT20Det\")  # same as before\n\n# Look only inside the *train* split where images + gt are guaranteed\ntrain_seqs = sorted([p for p in (DATA_DIR/'train').iterdir() if p.is_dir()])\nprint(\"Train sequences:\", [p.name for p in train_seqs])\n\n# Pick the first sequence that actually contains frames\nsample_img = None\nfor seq in train_seqs:\n    imgs = glob.glob(f\"{seq}/img1/*.jpg\") or glob.glob(f\"{seq}/img1/*.png\")\n    if imgs:\n        sample_img = random.choice(imgs)\n        gt_file   = seq/'gt/gt.txt'\n        break\n\nprint(\"Example frame file:\", sample_img)\nprint(\"Ground-truth file :\", gt_file)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T19:22:11.715606Z","iopub.execute_input":"2025-04-28T19:22:11.715886Z","iopub.status.idle":"2025-04-28T19:22:11.746508Z","shell.execute_reply.started":"2025-04-28T19:22:11.715865Z","shell.execute_reply":"2025-04-28T19:22:11.745775Z"}},"outputs":[{"name":"stdout","text":"Train sequences: ['MOT20-01', 'MOT20-02', 'MOT20-03', 'MOT20-05']\nExample frame file: /kaggle/input/data-set-s/MOT20Det/train/MOT20-01/img1/000155.jpg\nGround-truth file : /kaggle/input/data-set-s/MOT20Det/train/MOT20-01/gt/gt.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import shutil, pandas as pd, cv2, pathlib, tqdm, glob\n\nSRC_ROOT = pathlib.Path(\"/kaggle/input/data-set-s/MOT20Det/train\")   # read-only\nDST_ROOT = pathlib.Path(\"/kaggle/working/MOT20Det/train\")           # writeable\nDST_ROOT.mkdir(parents=True, exist_ok=True)\n\n# 1. Copy every sequence so we have write access\nfor seq in SRC_ROOT.iterdir():\n    if not seq.is_dir(): \n        continue\n    dst_seq = DST_ROOT/seq.name\n    if not dst_seq.exists():                     # skip if you already copied once\n        print(f\"Copying {seq.name} â†’ working...\")\n        shutil.copytree(seq, dst_seq)            # 2â€“3 min\n\n# 2. Create YOLO label files next to the copied images\nSEQ_DIRS = sorted([p for p in DST_ROOT.iterdir() if p.is_dir()])\n\nfor seq_dir in tqdm.tqdm(SEQ_DIRS, desc=\"Label-conversion\"):\n    gt_path = seq_dir / \"gt\" / \"gt.txt\"\n    gt = pd.read_csv(gt_path, header=None)\n    gt.columns = [\"frame\",\"tid\",\"x\",\"y\",\"w\",\"h\",\"conf\",\"cls\",\"vis\"]\n\n    # get resolution from first frame\n    first_img = next((seq_dir/\"img1\").glob(\"*.jpg\"))\n    H, W, _ = cv2.imread(str(first_img)).shape\n\n    for f_idx, rows in gt.groupby(\"frame\"):\n        label_path = seq_dir / \"img1\" / f\"{int(f_idx):06d}.txt\"\n        with open(label_path, \"w\") as f:\n            for _, r in rows.iterrows():\n                cx = (r.x + r.w/2) / W\n                cy = (r.y + r.h/2) / H\n                bw = r.w / W\n                bh = r.h / H\n                f.write(f\"0 {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T19:22:14.942043Z","iopub.execute_input":"2025-04-28T19:22:14.942331Z","iopub.status.idle":"2025-04-28T19:25:28.645444Z","shell.execute_reply.started":"2025-04-28T19:22:14.942311Z","shell.execute_reply":"2025-04-28T19:25:28.644689Z"}},"outputs":[{"name":"stdout","text":"Copying MOT20-02 â†’ working...\nCopying MOT20-03 â†’ working...\nCopying MOT20-01 â†’ working...\nCopying MOT20-05 â†’ working...\n","output_type":"stream"},{"name":"stderr","text":"Label-conversion: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:11<00:00, 17.84s/it]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ------------------------------------------------------------\n# 1. Define split and make symlinks  (â‰ˆ 1 second, no disk copy)\n# ------------------------------------------------------------\nimport os, pathlib, subprocess\n\nROOT = pathlib.Path(\"/kaggle/working/MOT20Det/train\")   # where the sequences were copied\nSPLIT_ROOT = pathlib.Path(\"/kaggle/working/mot20_split\")\n(SPLIT_ROOT / \"train\").mkdir(parents=True, exist_ok=True)\n(SPLIT_ROOT / \"val\").mkdir(parents=True, exist_ok=True)\n\nTRAIN_SEQS = [\"MOT20-01\", \"MOT20-03\"]\nVAL_SEQS   = [\"MOT20-02\"]          # keep MOT20-04 unseen for later testing\n\ndef safe_symlink(src, dst):\n    try:\n        os.symlink(src, dst)\n    except FileExistsError:\n        pass\n\nfor seq in TRAIN_SEQS:\n    safe_symlink(ROOT/seq, SPLIT_ROOT/\"train\"/seq)\nfor seq in VAL_SEQS:\n    safe_symlink(ROOT/seq, SPLIT_ROOT/\"val\"/seq)\n\nprint(\"Symlinks done:\")\nprint(\"  train â†’\", list((SPLIT_ROOT/\"train\").iterdir()))\nprint(\"  val   â†’\", list((SPLIT_ROOT/\"val\").iterdir()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T19:25:58.561136Z","iopub.execute_input":"2025-04-28T19:25:58.561639Z","iopub.status.idle":"2025-04-28T19:25:58.568974Z","shell.execute_reply.started":"2025-04-28T19:25:58.561614Z","shell.execute_reply":"2025-04-28T19:25:58.568273Z"}},"outputs":[{"name":"stdout","text":"Symlinks done:\n  train â†’ [PosixPath('/kaggle/working/mot20_split/train/MOT20-03'), PosixPath('/kaggle/working/mot20_split/train/MOT20-01')]\n  val   â†’ [PosixPath('/kaggle/working/mot20_split/val/MOT20-02')]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile mot20.yaml\npath: /kaggle/working/mot20_split      # DO NOT end with '/'\ntrain: train\nval: val\nnc: 1\nnames: [\"person\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T19:26:04.110860Z","iopub.execute_input":"2025-04-28T19:26:04.111134Z","iopub.status.idle":"2025-04-28T19:26:04.116008Z","shell.execute_reply.started":"2025-04-28T19:26:04.111115Z","shell.execute_reply":"2025-04-28T19:26:04.115256Z"}},"outputs":[{"name":"stdout","text":"Writing mot20.yaml\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!wget -q https://github.com/WongKinYiu/yolov9/releases/download/v0.2/yolov9s.pt -O yolov9s.pt\n!ls -lh yolov9s.pt     # quick check: file size ~ 27 MB\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:05:38.311062Z","iopub.execute_input":"2025-04-28T17:05:38.311354Z","iopub.status.idle":"2025-04-28T17:05:38.760043Z","shell.execute_reply.started":"2025-04-28T17:05:38.311320Z","shell.execute_reply":"2025-04-28T17:05:38.759210Z"}},"outputs":[{"name":"stdout","text":"-rw-r--r-- 1 root root 0 Apr 28 17:05 yolov9s.pt\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# delete the broken file if it exists\n!rm -f yolov9s.pt\n\n# use curl -L (follows redirects) and retry if the file is < 10 MB\n!curl -L -o yolov9s.pt https://github.com/WongKinYiu/yolov9/releases/download/v0.2/yolov9s.pt\n!ls -lh yolov9s.pt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T17:11:22.132107Z","iopub.execute_input":"2025-04-28T17:11:22.132786Z","iopub.status.idle":"2025-04-28T17:11:22.579477Z","shell.execute_reply.started":"2025-04-28T17:11:22.132757Z","shell.execute_reply":"2025-04-28T17:11:22.578265Z"}},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100     9  100     9    0     0    100      0 --:--:-- --:--:-- --:--:--   101\n-rw-r--r-- 1 root root 9 Apr 28 17:11 yolov9s.pt\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!pip install -q --upgrade ultralytics   # pulls the stable 8.x series\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T19:30:46.834158Z","iopub.execute_input":"2025-04-28T19:30:46.834770Z","iopub.status.idle":"2025-04-28T19:32:19.911592Z","shell.execute_reply.started":"2025-04-28T19:30:46.834744Z","shell.execute_reply":"2025-04-28T19:32:19.910556Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"PREMANI","metadata":{}},{"cell_type":"code","source":"import pathlib, sys, os\n\nMODEL_DIR = pathlib.Path(\"/kaggle/input/yolov8n/pytorch/demo/1\")\npt_files   = sorted(MODEL_DIR.rglob(\"*.pt\"))\nassert pt_files, f\"No .pt file found under {MODEL_DIR}\"\nWEIGHT_PATH = str(pt_files[0])\n\nprint(\"Using weight:\", WEIGHT_PATH)\nprint(\"Size:\", os.path.getsize(WEIGHT_PATH)/1e6, \"MB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T19:32:57.531919Z","iopub.execute_input":"2025-04-28T19:32:57.532521Z","iopub.status.idle":"2025-04-28T19:32:57.544494Z","shell.execute_reply.started":"2025-04-28T19:32:57.532494Z","shell.execute_reply":"2025-04-28T19:32:57.543707Z"}},"outputs":[{"name":"stdout","text":"Using weight: /kaggle/input/yolov8n/pytorch/demo/1/yolov8n.pt\nSize: 6.200696 MB\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"WEIGHT_PATH=\"/kaggle/input/yolov8n/pytorch/demo/1/yolov8n.pt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T19:33:03.039025Z","iopub.execute_input":"2025-04-28T19:33:03.039549Z","iopub.status.idle":"2025-04-28T19:33:03.042893Z","shell.execute_reply.started":"2025-04-28T19:33:03.039527Z","shell.execute_reply":"2025-04-28T19:33:03.042145Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"!yolo train task=detect model=/kaggle/input/yolov8n/pytorch/demo/1/yolov8n.pt data=mot20.yaml epochs=40 imgsz=800 batch=8 mosaic=1.0 close_mosaic=10 device=0 workers=4 project=mot20_yolov8n_user name=exp\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T19:33:06.563764Z","iopub.execute_input":"2025-04-28T19:33:06.564354Z","execution_failed":"2025-04-28T20:35:12.410Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nUltralytics 8.3.119 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/kaggle/input/yolov8n/pytorch/demo/1/yolov8n.pt, data=mot20.yaml, epochs=40, time=None, patience=100, batch=8, imgsz=800, save=True, save_period=-1, cache=False, device=0, workers=4, project=mot20_yolov8n_user, name=exp, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=mot20_yolov8n_user/exp\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 16.7MB/s]\nOverriding model.yaml nc=2 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nModel summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n\nTransferred 349/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 69.3MB/s]\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3112.6Â±708.7 MB/s, size: 275.5 KB)\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/mot20_split/train/MOT20-01/img1... 2834 images, \u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/mot20_split/train/MOT20-01/img1.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1864.7Â±1375.0 MB/s, size: 422.2 KB)\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/mot20_split/val/MOT20-02/img1... 2782 images, 0 ba\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/mot20_split/val/MOT20-02/img1.cache\nPlotting labels to mot20_yolov8n_user/exp/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 800 train, 800 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mmot20_yolov8n_user/exp\u001b[0m\nStarting training for 40 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/40      7.23G      1.588      1.085       1.17        443        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/working/runs/detect/mot20_yolov8n_user/exp/weights\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-28T19:19:58.610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"hello \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T19:21:47.759773Z","iopub.execute_input":"2025-04-28T19:21:47.759998Z","iopub.status.idle":"2025-04-28T19:21:47.766946Z","shell.execute_reply.started":"2025-04-28T19:21:47.759980Z","shell.execute_reply":"2025-04-28T19:21:47.766361Z"}},"outputs":[{"name":"stdout","text":"hello \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2 am","metadata":{}},{"cell_type":"code","source":"%pip install -q \"ultralytics>=8.3.117\" cython filterpy scikit-image \\\n               opencv-python-headless==4.10.0.84 tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:42:03.289758Z","iopub.execute_input":"2025-04-28T20:42:03.290057Z","iopub.status.idle":"2025-04-28T20:43:17.438254Z","shell.execute_reply.started":"2025-04-28T20:42:03.290033Z","shell.execute_reply":"2025-04-28T20:43:17.437397Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import sys, torch, ultralytics as ul\nprint(\"Py:\", sys.version)\nprint(\"Torch:\", torch.__version__)\nprint(\"Ultralytics:\", ul.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:43:47.283271Z","iopub.execute_input":"2025-04-28T20:43:47.283830Z","iopub.status.idle":"2025-04-28T20:43:53.069638Z","shell.execute_reply.started":"2025-04-28T20:43:47.283803Z","shell.execute_reply":"2025-04-28T20:43:53.069000Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nPy: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\nTorch: 2.5.1+cu124\nUltralytics: 8.3.119\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2, torch, time, pathlib, collections, numpy as np\n\n# MOT20 root that you uploaded\nDATA_ROOT = pathlib.Path(\"/kaggle/input/data-set-s/MOT20Det\")\n\n# choose one sequence to demo (change â€˜MOT20-01â€™ â‡„ 02/03/04 when you like)\nSEQ_DIR = DATA_ROOT / \"train/MOT20-01/img1\"\n\n# inference device\nDEVICE = 0 if torch.cuda.is_available() else \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:45:48.347600Z","iopub.execute_input":"2025-04-28T20:45:48.348049Z","iopub.status.idle":"2025-04-28T20:45:48.437717Z","shell.execute_reply.started":"2025-04-28T20:45:48.348024Z","shell.execute_reply":"2025-04-28T20:45:48.436992Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model = YOLO(\"yolov8n.pt\")   # start with nano; upgrade to yolov8m/l if you still hit 30 FPS\nmodel.fuse()\nif DEVICE != \"cpu\":\n    model.model.half()       # half-precision only on GPU\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:46:05.074528Z","iopub.execute_input":"2025-04-28T20:46:05.074792Z","iopub.status.idle":"2025-04-28T20:46:05.871077Z","shell.execute_reply.started":"2025-04-28T20:46:05.074772Z","shell.execute_reply":"2025-04-28T20:46:05.870422Z"}},"outputs":[{"name":"stdout","text":"Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.25M/6.25M [00:00<00:00, 86.2MB/s]\n","output_type":"stream"},{"name":"stdout","text":"YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class LiveInventory:\n    \"\"\"Track IDs across frames and raise NEW / MISSING events.\"\"\"\n    def __init__(self, miss_tolerance=30):\n        self.last_seen   = {}          # id â†’ frame_idx\n        self.frame_idx   = 0\n        self.miss_tol    = miss_tolerance\n\n    def update(self, tracks):\n        present_ids = {int(t.id) for t in tracks if t.id is not None}\n\n        # NEW\n        new_ids = [tid for tid in present_ids if tid not in self.last_seen]\n\n        # refresh time-stamps\n        for tid in present_ids:\n            self.last_seen[tid] = self.frame_idx\n\n        # MISSING\n        missing_ids = [tid for tid, last in list(self.last_seen.items())\n                       if self.frame_idx - last > self.miss_tol]\n        for tid in missing_ids:\n            del self.last_seen[tid]\n\n        self.frame_idx += 1\n        return new_ids, missing_ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:46:31.998069Z","iopub.execute_input":"2025-04-28T20:46:31.998336Z","iopub.status.idle":"2025-04-28T20:46:32.003616Z","shell.execute_reply.started":"2025-04-28T20:46:31.998319Z","shell.execute_reply":"2025-04-28T20:46:32.002860Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def stream_imgs(seq_dir):\n    for img_path in sorted(seq_dir.glob(\"*.jpg\")):\n        yield cv2.imread(str(img_path)), img_path.name\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:46:43.685989Z","iopub.execute_input":"2025-04-28T20:46:43.686671Z","iopub.status.idle":"2025-04-28T20:46:43.690305Z","shell.execute_reply.started":"2025-04-28T20:46:43.686646Z","shell.execute_reply":"2025-04-28T20:46:43.689607Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def run_sequence(seq_dir, show=False):\n    inv  = LiveInventory(miss_tolerance=30)   # ~1 s at 30 FPS\n    t0   = time.time()\n\n    for frame, fname in stream_imgs(seq_dir):\n        # Ultralytics track API; persist=True keeps ID assignment continuous\n        results = model.track(frame,\n                              tracker=\"bytetrack.yaml\",\n                              persist=True,\n                              imgsz=800,\n                              device=DEVICE,\n                              verbose=False)\n\n        tracks = results[0].boxes\n\n        new_ids, missing_ids = inv.update(tracks)\n\n        # log events\n        if new_ids:\n            print(f\"[{inv.frame_idx:05}] NEW â†’ {new_ids}\")\n        if missing_ids:\n            print(f\"[{inv.frame_idx:05}] MISSING â†’ {missing_ids}\")\n\n        # optional preview (avoid VideoWriter for max FPS)\n        if show:\n            vis = results[0].plot()\n            cv2.putText(vis,\n                        f\"NEW:{len(new_ids)}  MISS:{len(missing_ids)}\",\n                        (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.8,(0,255,0),2)\n            cv2.imshow(\"live\", vis)\n            if cv2.waitKey(1) == 27: break  # Esc quits\n\n    dt = time.time() - t0\n    print(f\"\\nProcessed {inv.frame_idx} frames in {dt:.1f}s  â†’  {inv.frame_idx/dt:.2f} FPS\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:47:10.686272Z","iopub.execute_input":"2025-04-28T20:47:10.686945Z","iopub.status.idle":"2025-04-28T20:47:10.692924Z","shell.execute_reply.started":"2025-04-28T20:47:10.686920Z","shell.execute_reply":"2025-04-28T20:47:10.692386Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"run_sequence(SEQ_DIR, show=False)   # set show=True if you launch with a GUI (not in Kaggle)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:47:34.988370Z","iopub.execute_input":"2025-04-28T20:47:34.988835Z","iopub.status.idle":"2025-04-28T20:48:00.338840Z","shell.execute_reply.started":"2025-04-28T20:47:34.988809Z","shell.execute_reply":"2025-04-28T20:48:00.338063Z"}},"outputs":[{"name":"stdout","text":"\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\nCollecting lap>=0.5.12\n  Downloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from lap>=0.5.12) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->lap>=0.5.12) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->lap>=0.5.12) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->lap>=0.5.12) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.6->lap>=0.5.12) (2024.2.0)\nDownloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.7/1.7 MB 25.8 MB/s eta 0:00:00\nInstalling collected packages: lap\nSuccessfully installed lap-0.5.12\n\n\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 3.0s, installed 1 package: ['lap>=0.5.12']\nWARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n\n[00001] NEW â†’ [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n[00004] NEW â†’ [22]\n[00005] NEW â†’ [25]\n[00007] NEW â†’ [31]\n[00009] NEW â†’ [34]\n[00016] NEW â†’ [48]\n[00017] NEW â†’ [51]\n[00018] NEW â†’ [53]\n[00019] NEW â†’ [55]\n[00020] NEW â†’ [57, 59]\n[00022] NEW â†’ [65]\n[00024] NEW â†’ [70, 73]\n[00029] NEW â†’ [83]\n[00030] NEW â†’ [85]\n[00034] NEW â†’ [93]\n[00035] MISSING â†’ [11]\n[00036] NEW â†’ [100]\n[00037] MISSING â†’ [13]\n[00041] NEW â†’ [112]\n[00044] NEW â†’ [123]\n[00045] NEW â†’ [128, 125]\n[00046] MISSING â†’ [16, 31]\n[00049] MISSING â†’ [34]\n[00050] NEW â†’ [130, 131]\n[00052] NEW â†’ [133]\n[00054] NEW â†’ [134]\n[00054] MISSING â†’ [55]\n[00055] NEW â†’ [135]\n[00058] NEW â†’ [137]\n[00059] NEW â†’ [140]\n[00060] MISSING â†’ [48, 53]\n[00063] MISSING â†’ [73]\n[00066] NEW â†’ [158]\n[00068] MISSING â†’ [6, 14]\n[00070] NEW â†’ [168]\n[00073] NEW â†’ [173]\n[00073] MISSING â†’ [59]\n[00075] NEW â†’ [176]\n[00075] MISSING â†’ [100]\n[00079] NEW â†’ [189]\n[00079] MISSING â†’ [57]\n[00082] NEW â†’ [195]\n[00083] MISSING â†’ [125]\n[00087] MISSING â†’ [133]\n[00090] MISSING â†’ [85]\n[00093] NEW â†’ [216, 218]\n[00093] MISSING â†’ [22]\n[00095] MISSING â†’ [70, 93]\n[00097] NEW â†’ [225]\n[00097] MISSING â†’ [131]\n[00099] NEW â†’ [229]\n[00100] NEW â†’ [232]\n[00101] MISSING â†’ [5]\n[00102] NEW â†’ [235]\n[00103] NEW â†’ [237]\n[00104] NEW â†’ [238]\n[00104] MISSING â†’ [137]\n[00107] MISSING â†’ [9]\n[00109] MISSING â†’ [25]\n[00116] NEW â†’ [261, 262]\n[00117] MISSING â†’ [112]\n[00120] NEW â†’ [278]\n[00121] NEW â†’ [279]\n[00121] MISSING â†’ [176]\n[00123] NEW â†’ [284]\n[00124] NEW â†’ [287]\n[00126] MISSING â†’ [128]\n[00128] NEW â†’ [300, 302]\n[00129] MISSING â†’ [225]\n[00131] NEW â†’ [313]\n[00131] MISSING â†’ [83]\n[00132] NEW â†’ [321]\n[00133] NEW â†’ [322]\n[00134] NEW â†’ [324]\n[00138] MISSING â†’ [237, 238]\n[00139] NEW â†’ [334]\n[00141] MISSING â†’ [235]\n[00143] NEW â†’ [342]\n[00144] MISSING â†’ [189]\n[00147] MISSING â†’ [65]\n[00149] MISSING â†’ [158]\n[00152] MISSING â†’ [12, 261]\n[00154] NEW â†’ [372]\n[00157] NEW â†’ [379]\n[00157] MISSING â†’ [195, 262]\n[00158] NEW â†’ [384]\n[00159] MISSING â†’ [279]\n[00162] NEW â†’ [398]\n[00164] NEW â†’ [400]\n[00166] MISSING â†’ [278]\n[00169] MISSING â†’ [173]\n[00171] MISSING â†’ [321]\n[00174] NEW â†’ [418, 419]\n[00178] NEW â†’ [427, 428]\n[00180] NEW â†’ [438]\n[00181] NEW â†’ [443]\n[00185] NEW â†’ [450, 451]\n[00186] NEW â†’ [453]\n[00186] MISSING â†’ [300]\n[00187] NEW â†’ [454]\n[00190] NEW â†’ [462]\n[00191] MISSING â†’ [51]\n[00192] NEW â†’ [466, 467]\n[00194] NEW â†’ [470]\n[00196] MISSING â†’ [140]\n[00198] NEW â†’ [473, 478]\n[00199] MISSING â†’ [324]\n[00200] MISSING â†’ [287]\n[00201] NEW â†’ [487]\n[00204] NEW â†’ [497]\n[00204] MISSING â†’ [342]\n[00205] MISSING â†’ [419]\n[00208] MISSING â†’ [216]\n[00209] MISSING â†’ [379]\n[00210] NEW â†’ [516]\n[00211] NEW â†’ [521]\n[00212] MISSING â†’ [123]\n[00213] NEW â†’ [527]\n[00213] MISSING â†’ [418]\n[00214] NEW â†’ [533]\n[00217] NEW â†’ [551]\n[00217] MISSING â†’ [135]\n[00220] NEW â†’ [569]\n[00221] NEW â†’ [571]\n[00221] MISSING â†’ [400]\n[00222] MISSING â†’ [438]\n[00223] MISSING â†’ [454]\n[00224] MISSING â†’ [302]\n[00226] NEW â†’ [585]\n[00227] MISSING â†’ [467, 470]\n[00228] MISSING â†’ [130, 450]\n[00229] NEW â†’ [591]\n[00229] MISSING â†’ [334]\n[00231] MISSING â†’ [453]\n[00233] NEW â†’ [604]\n[00234] NEW â†’ [607]\n[00234] MISSING â†’ [398]\n[00235] NEW â†’ [611]\n[00236] NEW â†’ [615]\n[00237] NEW â†’ [620]\n[00239] NEW â†’ [624]\n[00239] MISSING â†’ [427]\n[00241] NEW â†’ [628]\n[00241] MISSING â†’ [451]\n[00242] NEW â†’ [630]\n[00247] NEW â†’ [641]\n[00247] MISSING â†’ [497]\n[00248] NEW â†’ [643]\n[00248] MISSING â†’ [516]\n[00249] NEW â†’ [646]\n[00250] NEW â†’ [651]\n[00252] NEW â†’ [658]\n[00254] MISSING â†’ [527]\n[00256] MISSING â†’ [8]\n[00257] NEW â†’ [666]\n[00257] MISSING â†’ [533]\n[00259] MISSING â†’ [585]\n[00262] NEW â†’ [671]\n[00262] MISSING â†’ [384, 478]\n[00264] NEW â†’ [674]\n[00265] MISSING â†’ [521]\n[00266] NEW â†’ [676]\n[00267] NEW â†’ [681]\n[00268] NEW â†’ [684]\n[00268] MISSING â†’ [569]\n[00269] NEW â†’ [690]\n[00271] MISSING â†’ [607]\n[00275] MISSING â†’ [630]\n[00276] NEW â†’ [701]\n[00276] MISSING â†’ [604, 624]\n[00277] MISSING â†’ [4]\n[00278] NEW â†’ [710]\n[00278] MISSING â†’ [428, 571]\n[00281] NEW â†’ [720]\n[00283] NEW â†’ [723]\n[00286] NEW â†’ [730]\n[00286] MISSING â†’ [487]\n[00288] NEW â†’ [736]\n[00290] NEW â†’ [743, 744]\n[00290] MISSING â†’ [641]\n[00294] NEW â†’ [750]\n[00294] MISSING â†’ [643]\n[00295] NEW â†’ [753, 755]\n[00295] MISSING â†’ [658, 666]\n[00298] MISSING â†’ [676]\n[00300] NEW â†’ [762]\n[00302] MISSING â†’ [681]\n[00303] MISSING â†’ [620]\n[00304] NEW â†’ [772]\n[00306] MISSING â†’ [671]\n[00309] MISSING â†’ [684]\n[00311] MISSING â†’ [701]\n[00312] MISSING â†’ [134, 551]\n[00313] NEW â†’ [794]\n[00314] MISSING â†’ [322]\n[00315] MISSING â†’ [710]\n[00319] NEW â†’ [804]\n[00320] MISSING â†’ [674]\n[00321] NEW â†’ [808]\n[00322] MISSING â†’ [736]\n[00324] NEW â†’ [815]\n[00324] MISSING â†’ [229, 743, 744]\n[00326] NEW â†’ [820, 823]\n[00330] MISSING â†’ [755]\n[00331] MISSING â†’ [646]\n[00333] NEW â†’ [832]\n[00334] NEW â†’ [835]\n[00335] NEW â†’ [837]\n[00336] NEW â†’ [841, 842, 843]\n[00336] MISSING â†’ [651]\n[00339] NEW â†’ [845]\n[00340] NEW â†’ [846]\n[00343] MISSING â†’ [218]\n[00345] NEW â†’ [859]\n[00345] MISSING â†’ [750, 772]\n[00349] MISSING â†’ [1]\n[00350] MISSING â†’ [2, 168, 753]\n[00353] NEW â†’ [874]\n[00357] NEW â†’ [887, 888]\n[00359] MISSING â†’ [794]\n[00361] NEW â†’ [892]\n[00361] MISSING â†’ [615]\n[00363] MISSING â†’ [815]\n[00364] MISSING â†’ [313]\n[00368] NEW â†’ [903]\n[00369] MISSING â†’ [591]\n[00370] NEW â†’ [908]\n[00370] MISSING â†’ [808]\n[00371] NEW â†’ [913]\n[00373] MISSING â†’ [846]\n[00374] NEW â†’ [919]\n[00376] MISSING â†’ [845]\n[00377] NEW â†’ [928]\n[00377] MISSING â†’ [7, 762]\n[00378] MISSING â†’ [10, 837]\n[00379] MISSING â†’ [232]\n[00380] NEW â†’ [949]\n[00381] NEW â†’ [952, 955]\n[00382] MISSING â†’ [820]\n[00384] MISSING â†’ [15]\n[00387] NEW â†’ [972]\n[00388] NEW â†’ [977]\n[00389] NEW â†’ [978]\n[00389] MISSING â†’ [874]\n[00390] NEW â†’ [980]\n[00393] MISSING â†’ [888]\n[00394] NEW â†’ [985]\n[00394] MISSING â†’ [3, 723]\n[00396] MISSING â†’ [887, 892]\n[00397] MISSING â†’ [466]\n[00399] MISSING â†’ [859]\n[00401] MISSING â†’ [372, 842]\n[00402] NEW â†’ [993]\n[00407] NEW â†’ [998, 999, 1000]\n[00410] NEW â†’ [1006]\n[00410] MISSING â†’ [832]\n[00417] NEW â†’ [1018]\n[00419] MISSING â†’ [949, 977]\n[00420] NEW â†’ [1024]\n[00421] NEW â†’ [1027]\n[00421] MISSING â†’ [928, 972, 980]\n[00423] NEW â†’ [1031]\n[00424] MISSING â†’ [908]\n[00427] NEW â†’ [1036]\n[00427] MISSING â†’ [952]\n[00429] NEW â†’ [1041, 1043, 1044]\n\nProcessed 429 frames in 25.3s  â†’  16.93 FPS\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from pathlib import Path\nimport shutil, cv2, tqdm\n\nSRC = Path(\"/kaggle/input/data-set-s/MOT20Det\")          # read-only Kaggle dir\nDST = Path(\"/kaggle/working/mot20_yolo\")                 # writeable output\n(DST/\"images/train\").mkdir(parents=True, exist_ok=True)\n(DST/\"images/val\").mkdir(parents=True, exist_ok=True)\n(DST/\"labels/train\").mkdir(parents=True, exist_ok=True)\n(DST/\"labels/val\").mkdir(parents=True, exist_ok=True)\n\n# choose one sequence for validation (here MOT20-04); you can change this split\nVAL_SEQ = \"MOT20-04\"\n\ndef mot_to_yolo_line(x, y, w, h, img_w, img_h, cls=0):\n    # convert absolute box to YOLO (cx,cy,w,h) normalized 0-1\n    cx = (x + w/2) / img_w\n    cy = (y + h/2) / img_h\n    return f\"{cls} {cx:.6f} {cy:.6f} {w/img_w:.6f} {h/img_h:.6f}\\n\"\n\ndef process_sequence(seq_path, split):\n    imgs_out   = DST/f\"images/{split}\"\n    labels_out = DST/f\"labels/{split}\"\n\n    # load gt.txt into dict: frame_idx -> list of boxes\n    gt_file = seq_path/\"gt/gt.txt\"\n    boxes_per_frame = {}\n    with open(gt_file) as f:\n        for line in f:\n            f_id, obj_id, x, y, w, h, conf, cls, vis = map(float, line.split(','))\n            if int(cls) != 1:        # only 'person'\n                continue\n            boxes_per_frame.setdefault(int(f_id), []).append((x, y, w, h))\n\n    # iterate images\n    img_dir = seq_path/\"img1\"\n    for img_path in sorted(img_dir.glob(\"*.jpg\")):\n        frame_id = int(img_path.stem)\n        img = cv2.imread(str(img_path))\n        h, w = img.shape[:2]\n\n        # write label file\n        label_lines = []\n        for (x, y, bw, bh) in boxes_per_frame.get(frame_id, []):\n            label_lines.append(mot_to_yolo_line(x, y, bw, bh, w, h))\n        (labels_out/f\"{img_path.stem}.txt\").write_text(\"\".join(label_lines))\n\n        # copy image\n        shutil.copy2(img_path, imgs_out/img_path.name)\n\n# ---------- run over all sequences ----------\nfor seq in sorted((SRC/\"train\").iterdir()):\n    split = \"val\" if seq.name == VAL_SEQ else \"train\"\n    print(f\"âœï¸  {seq.name}  â†’  {split}\")\n    process_sequence(seq, split)\n\nprint(\"âœ…  MOT20 conversion finished.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:55:11.387168Z","iopub.execute_input":"2025-04-28T20:55:11.387665Z","iopub.status.idle":"2025-04-28T20:58:12.647263Z","shell.execute_reply.started":"2025-04-28T20:55:11.387646Z","shell.execute_reply":"2025-04-28T20:58:12.646458Z"}},"outputs":[{"name":"stdout","text":"âœï¸  MOT20-01  â†’  train\nâœï¸  MOT20-02  â†’  train\nâœï¸  MOT20-03  â†’  train\nâœï¸  MOT20-05  â†’  train\nâœ…  MOT20 conversion finished.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"%%bash\ncat > /kaggle/working/mot20.yaml <<'EOF'\npath: /kaggle/working/mot20_yolo\ntrain: images/train\nval: images/val\n\nnc: 1\nnames: ['person']\nEOF\necho \"âœ…  mot20.yaml written.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:58:57.469115Z","iopub.execute_input":"2025-04-28T20:58:57.469831Z","iopub.status.idle":"2025-04-28T20:58:57.482507Z","shell.execute_reply.started":"2025-04-28T20:58:57.469806Z","shell.execute_reply":"2025-04-28T20:58:57.481989Z"}},"outputs":[{"name":"stdout","text":"âœ…  mot20.yaml written.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Split MOT20 YOLO folder into train / val\n# root layout BEFORE running this cell:\n#   mot20_yolo/\n#      images/train/*.jpg\n#      labels/train/*.txt\n# AFTER running:\n#      images/val/*.jpg\n#      labels/val/*.txt\n# -------------------------------------------------------------------------\nfrom pathlib import Path\nimport random, shutil, yaml, math\n\n# CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nROOT       = Path(\"/kaggle/working/mot20_yolo\")  # change if you saved elsewhere\nVAL_RATIO  = 0.20        # 10 % validation\nRAND_SEED  = 42          # reproducible split\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nrandom.seed(RAND_SEED)\n\nimgs_train_dir  = ROOT/\"images/train\"\nlabels_train_dir= ROOT/\"labels/train\"\nimgs_val_dir    = ROOT/\"images/val\";    imgs_val_dir.mkdir(parents=True, exist_ok=True)\nlabels_val_dir  = ROOT/\"labels/val\";    labels_val_dir.mkdir(parents=True, exist_ok=True)\n\n# 1ï¸âƒ£  pick random subset\nimg_paths = sorted(imgs_train_dir.glob(\"*.jpg\"))\nn_val     = max(1, math.floor(len(img_paths)*VAL_RATIO))\nval_imgs  = set(random.sample(img_paths, n_val))\n\n# 2ï¸âƒ£  move images + matching labels\nfor img_path in img_paths:\n    stem = img_path.stem\n    lbl_path = labels_train_dir/f\"{stem}.txt\"\n    if img_path in val_imgs:\n        shutil.move(img_path, imgs_val_dir/img_path.name)\n        shutil.move(lbl_path, labels_val_dir/lbl_path.name)\n\nprint(f\"âœ…  Moved {n_val} images to validation split \"\n      f\"({len(img_paths)-n_val} remain in training).\")\n\n# 3ï¸âƒ£  rewrite YAML so Ultralytics knows the paths\nyaml_path = ROOT.parent/\"mot20.yaml\"   # e.g. /kaggle/working/mot20.yaml\ndata = dict(\n    path=str(ROOT),\n    train=\"images/train\",\n    val=\"images/val\",\n    nc=1,\n    names=[\"person\"]\n)\nyaml_path.write_text(yaml.dump(data, sort_keys=False))\nprint(f\"âœ…  Updated {yaml_path}\")\n\n# quick sanity check\nprint(\"\\nSample counts:\")\nprint(\" train images:\", len(list((ROOT/'images/train').glob('*.jpg'))))\nprint(\" val images  :\", len(list((ROOT/'images/val').glob('*.jpg'))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T21:08:15.275188Z","iopub.execute_input":"2025-04-28T21:08:15.275581Z","iopub.status.idle":"2025-04-28T21:08:15.370797Z","shell.execute_reply.started":"2025-04-28T21:08:15.275554Z","shell.execute_reply":"2025-04-28T21:08:15.370228Z"}},"outputs":[{"name":"stdout","text":"âœ…  Moved 663 images to validation split (2652 remain in training).\nâœ…  Updated /kaggle/working/mot20.yaml\n\nSample counts:\n train images: 2652\n val images  : 663\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!yolo train model=yolov8n.pt data=/kaggle/working/mot20.yaml \\\n            epochs=40 imgsz=800 batch=16 device=0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T21:08:25.982258Z","iopub.execute_input":"2025-04-28T21:08:25.982789Z","iopub.status.idle":"2025-04-28T22:03:37.959113Z","shell.execute_reply.started":"2025-04-28T21:08:25.982766Z","shell.execute_reply":"2025-04-28T22:03:37.958282Z"}},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.119 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/working/mot20.yaml, epochs=40, time=None, patience=100, batch=16, imgsz=800, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nModel summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3719.1Â±955.0 MB/s, size: 408.1 KB)\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/mot20_yolo/labels/train... 2652 images, 0 backgr\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/mot20_yolo/labels/train.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1578.1Â±1306.6 MB/s, size: 408.0 KB)\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/mot20_yolo/labels/val... 663 images, 0 backgrounds\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/mot20_yolo/labels/val.cache\nPlotting labels to runs/detect/train3/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 800 train, 800 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train3\u001b[0m\nStarting training for 40 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       1/40      10.3G      1.743      1.677      1.348       4644        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.896      0.802      0.902      0.569\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       2/40      10.5G      1.442     0.8016       1.15       3639        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.917      0.869      0.933      0.599\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       3/40      10.4G      1.345     0.6977      1.122       3524        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.942      0.862      0.941      0.602\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       4/40      9.98G      1.295     0.6544        1.1       3506        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.945      0.892      0.954      0.641\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       5/40      12.2G      1.255     0.6188      1.084       3079        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.951      0.894      0.959       0.66\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       6/40      13.5G      1.252     0.6059      1.081       4308        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.943      0.893      0.957      0.628\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       7/40      9.67G      1.238     0.5927      1.076       3426        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.959      0.901      0.963      0.669\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       8/40      10.6G      1.206      0.576      1.061       3801        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.959      0.902      0.964      0.683\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n       9/40      11.4G      1.207     0.5731      1.057       3322        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.959      0.908      0.967      0.683\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      10/40      14.2G      1.164     0.5558      1.043       4256        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.962      0.907      0.967      0.691\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      11/40      10.6G      1.151     0.5461      1.039       4058        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.962       0.91      0.968      0.692\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      12/40      9.92G      1.138     0.5353      1.032       2971        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.964      0.911      0.969      0.704\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      13/40      12.4G      1.133     0.5315      1.031       4197        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.962      0.918      0.971       0.71\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      14/40      11.2G      1.121     0.5249      1.025       3950        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.965      0.919      0.972      0.715\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      15/40      11.4G      1.103     0.5178      1.018       3191        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.964      0.918      0.972      0.713\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      16/40      9.39G      1.093     0.5132      1.014       4486        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.964      0.921      0.973      0.718\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      17/40      14.3G      1.084     0.5079      1.012       4118        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.962      0.922      0.973      0.711\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      18/40      14.3G      1.072      0.503      1.006       3685        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.965      0.923      0.974      0.726\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      19/40      9.68G      1.057     0.4952          1       3565        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.967      0.923      0.974      0.733\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      20/40      14.3G      1.056     0.4951      1.004       3900        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.965      0.926      0.975      0.732\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      21/40        14G      1.043     0.4902     0.9983       3907        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.965      0.925      0.976      0.734\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      22/40      10.4G      1.044     0.4894     0.9986       4306        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.968      0.925      0.976      0.737\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      23/40      12.3G      1.038     0.4863     0.9971       4092        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.969      0.926      0.976      0.744\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      24/40      9.16G      1.018     0.4783     0.9879       3419        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.967      0.929      0.977      0.746\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      25/40      14.3G      1.011     0.4757     0.9846       4833        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.967      0.929      0.977      0.742\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      26/40      14.3G      1.008     0.4733     0.9836       4146        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432       0.97       0.93      0.978      0.754\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      27/40      12.7G     0.9948     0.4693     0.9792       4029        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.968      0.931      0.977      0.752\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      28/40      9.15G     0.9911     0.4661     0.9759       3942        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.971       0.93      0.978      0.757\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      29/40      13.5G     0.9991     0.4677      0.978       4423        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.971      0.932      0.978      0.758\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      30/40      11.4G     0.9788     0.4603     0.9735       4362        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432       0.97      0.932      0.979       0.76\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      31/40      5.39G     0.9625     0.4561     0.9892       2100        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432       0.97      0.931      0.978      0.757\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      32/40      7.24G     0.9256     0.4345     0.9761       2115        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.971      0.933      0.979      0.763\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      33/40      7.24G      0.914     0.4286     0.9723       2031        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432       0.97      0.936      0.979      0.767\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      34/40      7.24G     0.9039     0.4247     0.9657       2179        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.972      0.935       0.98      0.772\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      35/40      7.24G      0.897     0.4224      0.964       2170        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432       0.97      0.938       0.98      0.772\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      36/40      7.24G      0.885     0.4174     0.9604       2064        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.972      0.937       0.98      0.776\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      37/40      7.24G     0.8791     0.4152     0.9558       1990        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.972      0.939      0.981      0.777\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      38/40      7.24G     0.8752     0.4136     0.9548       1936        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.974      0.937      0.981       0.78\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      39/40      7.24G     0.8648     0.4089     0.9527       2164        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.974      0.938      0.981      0.781\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      40/40      7.24G     0.8618     0.4079     0.9496       2158        800: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.973      0.939      0.981      0.781\n\n40 epochs completed in 0.901 hours.\nOptimizer stripped from runs/detect/train3/weights/last.pt, 6.2MB\nOptimizer stripped from runs/detect/train3/weights/best.pt, 6.2MB\n\nValidating runs/detect/train3/weights/best.pt...\nUltralytics 8.3.119 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        663     129432      0.974      0.938      0.981      0.781\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\nSpeed: 0.2ms preprocess, 2.3ms inference, 0.0ms loss, 1.7ms postprocess per image\nResults saved to \u001b[1mruns/detect/train3\u001b[0m\nğŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# One-time inside a new code-cell\n!pip install --quiet python-docx gitpython\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:07:24.446456Z","iopub.execute_input":"2025-04-28T22:07:24.447227Z","iopub.status.idle":"2025-04-28T22:07:27.758109Z","shell.execute_reply.started":"2025-04-28T22:07:24.447200Z","shell.execute_reply":"2025-04-28T22:07:27.757317Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import shutil, pathlib, json, subprocess, os, cv2, time, torch\nfrom ultralytics import YOLO\n\nROOT = pathlib.Path('/kaggle/working')\nRUNS = ROOT/'runs/detect'          # Ultralytics default\nBEST = next((RUNS.glob('train*/weights/best.pt'))).__str__()  # first best.pt\n\n# Make project folder\nPRJ = ROOT/'mot20_rt'\n(PRJ/'src').mkdir(parents=True, exist_ok=True)\n(PRJ/'assets').mkdir(exist_ok=True)\n\nshutil.copy2(BEST, PRJ/'yolov8_mot20.pt')\nprint(\"âœ… copied weight to:\", PRJ/'yolov8_mot20.pt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:08:22.316869Z","iopub.execute_input":"2025-04-28T22:08:22.317628Z","iopub.status.idle":"2025-04-28T22:08:22.330246Z","shell.execute_reply.started":"2025-04-28T22:08:22.317601Z","shell.execute_reply":"2025-04-28T22:08:22.329520Z"}},"outputs":[{"name":"stdout","text":"âœ… copied weight to: /kaggle/working/mot20_rt/yolov8_mot20.pt\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"code = r\"\"\"\nimport argparse, cv2, time, pathlib, torch\nfrom ultralytics import YOLO\n\ndef main(opt):\n    device = 0 if torch.cuda.is_available() else 'cpu'\n    model  = YOLO(opt.weights).fuse().to(device)\n    if device != 'cpu': model.model.half()\n\n    cap = cv2.VideoCapture(opt.source)\n    fps_in = cap.get(cv2.CAP_PROP_FPS) or 25\n    w, h = int(cap.get(3)), int(cap.get(4))\n    out = cv2.VideoWriter(opt.out, cv2.VideoWriter_fourcc(*'mp4v'), fps_in, (w, h))\n\n    t0, frame = time.time(), 0\n    while True:\n        ok, im = cap.read()\n        if not ok: break\n        res = model.track(im, tracker=\"bytetrack.yaml\", persist=True, verbose=False)\n        out.write(res[0].plot())\n        frame += 1\n    cap.release(); out.release()\n    fps = frame/(time.time()-t0)\n    pathlib.Path('assets/fps.txt').write_text(f\"{fps:.2f}\\\\n\")\n    print(f\"Processed {frame} frames â†’ {fps:.2f} FPS\")\nif __name__ == \"__main__\":\n    p = argparse.ArgumentParser()\n    p.add_argument('--source', default='demo.mp4')\n    p.add_argument('--weights', default='yolov8_mot20.pt')\n    p.add_argument('--out', default='assets/output_demo.mp4')\n    main(p.parse_args())\n\"\"\"\n(PRJ/'src/rt_detect.py').write_text(code)\nprint(\"âœ… inference script saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:08:36.979668Z","iopub.execute_input":"2025-04-28T22:08:36.980010Z","iopub.status.idle":"2025-04-28T22:08:36.991285Z","shell.execute_reply.started":"2025-04-28T22:08:36.979984Z","shell.execute_reply":"2025-04-28T22:08:36.990489Z"}},"outputs":[{"name":"stdout","text":"âœ… inference script saved.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from pathlib import Path\nimport shutil\n\nPRJ = Path(\"/kaggle/working/mot20_rt\")   # folder that will hold your repo\nPRJ.mkdir(parents=True, exist_ok=True)\nprint(\"PROJECT DIR:\", PRJ)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:37:17.009599Z","iopub.execute_input":"2025-04-28T22:37:17.009885Z","iopub.status.idle":"2025-04-28T22:37:17.014913Z","shell.execute_reply.started":"2025-04-28T22:37:17.009866Z","shell.execute_reply":"2025-04-28T22:37:17.014263Z"}},"outputs":[{"name":"stdout","text":"PROJECT DIR: /kaggle/working/mot20_rt\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# copy best.pt only if it isn't there yet\nBEST = next(Path(\"runs/detect\").glob(\"*/weights/best.pt\"), None)\nassert BEST, \"âŒ best.pt not found under runs/detect\"\nshutil.copy2(BEST, PRJ/\"yolov8_mot20.pt\")\nprint(\"âœ… weight copied to\", PRJ/\"yolov8_mot20.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:37:30.669225Z","iopub.execute_input":"2025-04-28T22:37:30.669464Z","iopub.status.idle":"2025-04-28T22:37:30.684595Z","shell.execute_reply.started":"2025-04-28T22:37:30.669447Z","shell.execute_reply":"2025-04-28T22:37:30.683850Z"}},"outputs":[{"name":"stdout","text":"âœ… weight copied to /kaggle/working/mot20_rt/yolov8_mot20.pt\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"code = r\"\"\"\nimport argparse, cv2, time, pathlib, torch, sys\nfrom ultralytics import YOLO\n\ndef main():\n    p = argparse.ArgumentParser()\n    p.add_argument('--source',  default='demo.mp4')\n    p.add_argument('--weights', default='yolov8_mot20.pt')\n    p.add_argument('--out',     default='assets/output_demo.mp4')\n    # accept and ignore any unknown args Jupyter may tack on\n    opt, _ = p.parse_known_args()\n\n    device = 0 if torch.cuda.is_available() else 'cpu'\n\n    model = YOLO(opt.weights)\n    model.fuse()                 # in-place\n    model.to(device)\n    if device != 'cpu':\n        model.model.half()\n\n    cap = cv2.VideoCapture(opt.source)\n    if not cap.isOpened():\n        sys.exit(f\"âŒ cannot open video {opt.source}\")\n    fps_in = cap.get(cv2.CAP_PROP_FPS) or 25\n    w, h = int(cap.get(3)), int(cap.get(4))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    pathlib.Path(opt.out).parent.mkdir(parents=True, exist_ok=True)\n    out = cv2.VideoWriter(opt.out, fourcc, fps_in, (w, h))\n\n    t0, frames = time.time(), 0\n    while True:\n        ok, img = cap.read()\n        if not ok:\n            break\n        res = model.track(img, tracker=\"bytetrack.yaml\", persist=True, verbose=False)\n        out.write(res[0].plot())\n        frames += 1\n\n    cap.release(); out.release()\n    fps = frames / max(1e-6, (time.time() - t0))\n    pathlib.Path(\"assets\").mkdir(exist_ok=True)\n    pathlib.Path(\"assets/fps.txt\").write_text(f\"{fps:.2f}\\n\")\n    print(f\"Processed {frames} frames  â†’  {fps:.2f} FPS\")\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n(PRJ/'src').mkdir(parents=True, exist_ok=True)\n(PRJ/'src/rt_detect.py').write_text(code)\nprint(\"âœ… script written:\", PRJ/'src/rt_detect.py')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:37:53.011524Z","iopub.execute_input":"2025-04-28T22:37:53.012216Z","iopub.status.idle":"2025-04-28T22:37:53.017687Z","shell.execute_reply.started":"2025-04-28T22:37:53.012192Z","shell.execute_reply":"2025-04-28T22:37:53.017124Z"}},"outputs":[{"name":"stdout","text":"âœ… script written: /kaggle/working/mot20_rt/src/rt_detect.py\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import os, shutil\nfrom pathlib import Path\n\nPRJ = Path(\"/kaggle/working/mot20_rt\")\nPRJ.mkdir(parents=True, exist_ok=True)\n\n# make it visible to subsequent %%bash cells\nos.environ[\"PRJ\"] = str(PRJ)\nprint(\"PROJECT DIR:\", PRJ)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:40:38.607359Z","iopub.execute_input":"2025-04-28T22:40:38.607957Z","iopub.status.idle":"2025-04-28T22:40:38.612848Z","shell.execute_reply.started":"2025-04-28T22:40:38.607910Z","shell.execute_reply":"2025-04-28T22:40:38.612133Z"}},"outputs":[{"name":"stdout","text":"PROJECT DIR: /kaggle/working/mot20_rt\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"demo_mp4 = PRJ / \"demo.mp4\"\n\ncmd = [\n    \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n    \"-framerate\", \"25\",\n    \"-i\", \"/kaggle/input/data-set-s/MOT20Det/test/MOT20-04/img1/%06d.jpg\",\n    \"-t\", \"10\",\n    \"-vf\", \"format=yuv420p,scale=trunc(iw/2)*2:trunc(ih/2)*2\",\n    str(demo_mp4)\n]\nprint(\"Running FFmpeg â€¦\")\nsubprocess.run(cmd, check=True)\nprint(\"âœ… demo.mp4 ready:\", demo_mp4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:42:58.082840Z","iopub.execute_input":"2025-04-28T22:42:58.083224Z","iopub.status.idle":"2025-04-28T22:43:17.596659Z","shell.execute_reply.started":"2025-04-28T22:42:58.083183Z","shell.execute_reply":"2025-04-28T22:43:17.595996Z"}},"outputs":[{"name":"stdout","text":"Running FFmpeg â€¦\nâœ… demo.mp4 ready: /kaggle/working/mot20_rt/demo.mp4\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# make sure the weight file exists (copy once if needed)\nbest = next(Path(\"runs/detect\").glob(\"*/weights/best.pt\"), None)\nassert best, \"best.pt not found\"\nshutil.copy2(best, PRJ/\"yolov8_mot20.pt\")\n\n# call your script\nsubprocess.run([\n    \"python\", str(PRJ/\"src/rt_detect.py\"),\n    \"--source\", str(demo_mp4),\n    \"--weights\", str(PRJ/\"yolov8_mot20.pt\"),\n    \"--out\", str(PRJ/\"assets/output_demo.mp4\")\n], check=True)\n\n# extract one still for the report\nsubprocess.run([\n    \"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n    \"-i\", str(PRJ/\"assets/output_demo.mp4\"),\n    \"-vframes\", \"1\", str(PRJ/\"assets/sample_frame.jpg\")\n], check=True)\n\nprint(\"âœ… all artifacts created in\", PRJ/\"assets\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:43:29.896162Z","iopub.execute_input":"2025-04-28T22:43:29.896416Z","iopub.status.idle":"2025-04-28T22:44:05.981244Z","shell.execute_reply.started":"2025-04-28T22:43:29.896399Z","shell.execute_reply":"2025-04-28T22:44:05.980573Z"}},"outputs":[{"name":"stdout","text":"Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\nProcessed 250 frames  â†’  7.80 FPS\nâœ… all artifacts created in /kaggle/working/mot20_rt/assets\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"from pathlib import Path\nimport argparse, cv2, time, torch\nfrom ultralytics import YOLO\nimport collections\n\ndef main():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--source\",  default=\"demo.mp4\")\n    p.add_argument(\"--weights\", default=\"/kaggle/working/mot20_rt/yolov8_mot20.pt\")\n    p.add_argument(\"--out\",     default=\"assets/output_events.mp4\")\n    p.add_argument(\"--miss_tol\",type=int, default=30,\n                   help=\"frames before an ID is marked MISSING\")\n    opt, _ = p.parse_known_args()\n\n    device = 0 if torch.cuda.is_available() else \"cpu\"\n    model  = YOLO(opt.weights)\n    model.fuse(); model.to(device)\n    if device != \"cpu\":\n        model.model.half()\n\n    # â”€â”€ NEW / MISSING state holder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    last_seen = {}\n    frame_idx = 0\n\n    cap = cv2.VideoCapture(opt.source)\n    fps_in = cap.get(cv2.CAP_PROP_FPS) or 25\n    w, h   = int(cap.get(3)), int(cap.get(4))\n    out = cv2.VideoWriter(opt.out,\n                          cv2.VideoWriter_fourcc(*\"mp4v\"),\n                          fps_in, (w, h))\n\n    t0, frames = time.time(), 0\n    while True:\n        ok, img = cap.read()\n        if not ok:\n            break\n\n        res = model.track(img, tracker=\"bytetrack.yaml\",\n                          persist=True, verbose=False)\n        tracks = res[0].boxes\n\n        # ---- event logic -------------------------------------------------\n        current_ids = {int(t.id) for t in tracks if t.id is not None}\n        new_ids     = [tid for tid in current_ids if tid not in last_seen]\n        missing_ids = [tid for tid, last in list(last_seen.items())\n                       if frame_idx - last > opt.miss_tol]\n\n        # update last_seen\n        for tid in current_ids:\n            last_seen[tid] = frame_idx\n        for tid in missing_ids:\n            last_seen.pop(tid, None)\n\n        frame_idx += 1\n        frames += 1\n\n        canvas = res[0].plot()   # boxes+IDs\n        # draw event text top-left\n        txt = f\"NEW:{new_ids}  MISSING:{missing_ids}\"\n        cv2.putText(canvas, txt, (10,30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n                    (0,255,0), 2, cv2.LINE_AA)\n\n        out.write(canvas)\n\n    cap.release(); out.release()\n    fps = frames / max(1e-6, time.time()-t0)\n    Path(\"assets\").mkdir(exist_ok=True)\n    Path(\"assets/fps.txt\").write_text(f\"{fps:.2f}\\n\")\n    print(f\"Processed {frames} frames â†’ {fps:.2f} FPS\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:48:48.073724Z","iopub.execute_input":"2025-04-28T22:48:48.074117Z","iopub.status.idle":"2025-04-28T22:48:48.242164Z","shell.execute_reply.started":"2025-04-28T22:48:48.074092Z","shell.execute_reply":"2025-04-28T22:48:48.241370Z"}},"outputs":[{"name":"stdout","text":"Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\nProcessed 0 frames â†’ 0.00 FPS\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"from pathlib import Path\nimport os\n\nPRJ = Path(\"/kaggle/working/mot20_rt\")          # project root\n(PRJ / \"src\").mkdir(parents=True, exist_ok=True)\n\ncode = r\"\"\"\nimport argparse, cv2, time, torch, pathlib, sys\nfrom ultralytics import YOLO\n\ndef main():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--source\",  required=True)\n    p.add_argument(\"--weights\", required=True)\n    p.add_argument(\"--out\",     required=True)\n    p.add_argument(\"--miss_tol\",type=int, default=30)\n    opt, _ = p.parse_known_args()\n\n    device = 0 if torch.cuda.is_available() else \"cpu\"\n    model  = YOLO(opt.weights)\n    model.fuse(); model.to(device)\n    if device != \"cpu\": model.model.half()\n\n    last_seen, frame_id = {}, 0\n    cap = cv2.VideoCapture(opt.source)\n    if not cap.isOpened():\n        sys.exit(f\"âŒ cannot open {opt.source}\")\n    fps_in = cap.get(cv2.CAP_PROP_FPS) or 25\n    w, h   = int(cap.get(3)), int(cap.get(4))\n    out_v  = cv2.VideoWriter(opt.out,\n              cv2.VideoWriter_fourcc(*\"mp4v\"), fps_in, (w, h))\n\n    t0 = time.time()\n    while True:\n        ok, img = cap.read()\n        if not ok: break\n        res = model.track(img, tracker=\"bytetrack.yaml\", persist=True, verbose=False)\n        ids = {int(b.id) for b in res[0].boxes if b.id is not None}\n\n        new_ids  = [i for i in ids if i not in last_seen]\n        gone_ids = [i for i, last in list(last_seen.items())\n                    if frame_id - last > opt.miss_tol]\n\n        for i in ids: last_seen[i] = frame_id\n        for i in gone_ids: last_seen.pop(i, None)\n        frame_id += 1\n\n        frame = res[0].plot()\n        txt = f\"NEW:{new_ids}  MISSING:{gone_ids}\"\n        cv2.putText(frame, txt, (10,30), cv2.FONT_HERSHEY_SIMPLEX,\n                    0.7, (0,255,0), 2, cv2.LINE_AA)\n        out_v.write(frame)\n\n    cap.release(); out_v.release()\n    fps = frame_id / max(1e-6, time.time()-t0)\n    pathlib.Path(\"assets\").mkdir(exist_ok=True)\n    pathlib.Path(\"assets/fps.txt\").write_text(f\"{fps:.2f}\\n\")\n    print(f\"Done â€“ {frame_id} frames â†’ {fps:.2f} FPS\")\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n(PRJ/\"src/rt_detect_events.py\").write_text(code)\nprint(\"âœ…  rt_detect_events.py created at\", PRJ/\"src/rt_detect_events.py\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T22:51:26.428745Z","iopub.execute_input":"2025-04-28T22:51:26.429065Z","iopub.status.idle":"2025-04-28T22:51:26.435727Z","shell.execute_reply.started":"2025-04-28T22:51:26.429042Z","shell.execute_reply":"2025-04-28T22:51:26.435047Z"}},"outputs":[{"name":"stdout","text":"âœ…  rt_detect_events.py created at /kaggle/working/mot20_rt/src/rt_detect_events.py\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import subprocess, os, shutil\nfrom pathlib import Path\n\nPRJ = Path(\"/kaggle/working/mot20_rt\")\n\n# ensure weight is present\nbest = next(Path(\"runs/detect\").glob(\"*/weights/best.pt\"), None)\nassert best, \"best.pt not found\"\nshutil.copy2(best, PRJ/\"yolov8_mot20.pt\")\n\n# make 15-s clip from unseen TEST split\ntest_clip = PRJ/\"demo_test.mp4\"\nsubprocess.run([\n    \"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\n    \"-framerate\",\"25\",\n    \"-i\",\"/kaggle/input/data-set-s/MOT20Det/test/MOT20-04/img1/%06d.jpg\",\n    \"-t\",\"15\",\n    \"-vf\",\"format=yuv420p,scale=trunc(iw/2)*2:trunc(ih/2)*2\",\n    str(test_clip)\n], check=True)\n\n# run detector with NEW/MISSING overlay\nsubprocess.run([\n    \"python\", str(PRJ/\"src/rt_detect_events.py\"),\n    \"--source\",  str(test_clip),\n    \"--weights\", str(PRJ/\"yolov8_mot20.pt\"),\n    \"--out\",     str(PRJ/\"assets/output_events.mp4\")\n], check=True)\n\n# grab one still\nsubprocess.run([\n    \"ffmpeg\",\"-y\",\"-hide_banner\",\"-loglevel\",\"error\",\n    \"-i\", str(PRJ/\"assets/output_events.mp4\"),\n    \"-vframes\",\"1\", str(PRJ/\"assets/sample_events.jpg\")\n], check=True)\n\nprint(\"\\nğŸ‰  All artifacts ready in\", PRJ/\"assets\")\nprint(\"    ->\", list((PRJ/\"assets\").iterdir()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T23:23:04.544882Z","iopub.execute_input":"2025-04-28T23:23:04.545707Z","iopub.status.idle":"2025-04-28T23:24:21.409337Z","shell.execute_reply.started":"2025-04-28T23:23:04.545680Z","shell.execute_reply":"2025-04-28T23:24:21.408481Z"}},"outputs":[{"name":"stdout","text":"Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\nDone â€“ 375 frames â†’ 8.15 FPS\n\nğŸ‰  All artifacts ready in /kaggle/working/mot20_rt/assets\n    -> [PosixPath('/kaggle/working/mot20_rt/assets/output_demo.mp4'), PosixPath('/kaggle/working/mot20_rt/assets/sample_frame.jpg'), PosixPath('/kaggle/working/mot20_rt/assets/sample_events.jpg'), PosixPath('/kaggle/working/mot20_rt/assets/output_events.mp4')]\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
